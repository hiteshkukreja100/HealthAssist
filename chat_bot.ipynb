{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-mQCYgeJ97u",
        "outputId": "4d825f79-c073-4030-f5bb-56ab41d14f36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/158.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m153.6/158.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Used to securely store your API key\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "C3iLbNa9KCGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Or use `os.getenv('GOOGLE_API_KEY')` to fetch an environment variable.\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "jAx7L3zpKLLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel('gemini-pro')"
      ],
      "metadata": {
        "id": "RZh0WUgOTZEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQtv054oZhgO",
        "outputId": "3975e588-2192-4a35-f8c6-f31bc00dd74b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "import pathlib\n",
        "import textwrap\n",
        "import markdown\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "# # Configure the API key for Google Generative AI\n",
        "# genai.configure(api_key=os.getenv(\"AIzaSyB0zbbq3Ck9NTlsa27mRvz4z3xG8FjBuPs\"))\n",
        "\n",
        "# # Function to load Gemini Pro model and get responses\n",
        "# model = genai.GenerativeModel(\"gemini-pro\")\n",
        "chat = model.start_chat(history=[])\n",
        "\n",
        "import nltk  # Import nltk for tokenization\n",
        "\n",
        "def get_gemini_response(question):\n",
        "  response = chat.send_message(question, stream=True)\n",
        "  for chunk in response:\n",
        "    # Tokenize the text (using nltk)\n",
        "    tokens = nltk.word_tokenize(chunk.text)\n",
        "\n",
        "    # Check if token count exceeds limit\n",
        "    if len(tokens) > 150:\n",
        "      # Truncate the text to 200 tokens (adjust slightly if needed)\n",
        "      truncated_text = \" \".join(tokens[:145]) + \"...\"\n",
        "    else:\n",
        "      truncated_text = chunk.text\n",
        "\n",
        "    yield truncated_text  # Yield the truncated text\n",
        "\n",
        "def main():\n",
        "    print(\"Welcome to the HealthAssist. Your AI Health Assistant!\")\n",
        "    chat_history = []\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() in ['exit', 'quit']:\n",
        "            print(\"Exiting the chat. Goodbye!\")\n",
        "            break\n",
        "        prompt = f\"Provide medically reviewed information related to the user's query and try to keep the response to the minimum: {user_input}\"\n",
        "        response = get_gemini_response(prompt)\n",
        "        # Add user query and response to chat history\n",
        "        chat_history.append((\"You\", user_input))\n",
        "        print(\"Health Assistant:\")\n",
        "        for truncated_text in response:\n",
        "            replaced_text = truncated_text.replace('*','')\n",
        "            print(replaced_text)\n",
        "            chat_history.append((\"Health Assistant\", truncated_text))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "id": "muT3HSzqw-Xy",
        "outputId": "b4caca63-12a5-4623-fba4-0c18f46ffea2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to the HealthAssist. Your AI Health Assistant!\n",
            "You: can you predict what i might be suffering from based on my symptoms\n",
            "Health Assistant:\n",
            "Predicting medical conditions based on symptoms alone is not possible. Accurate diagnosis requires a\n",
            " thorough medical evaluation by a qualified healthcare professional. Self-diagnosis can be misleading and potentially harmful. For reliable medical advice and treatment, it is crucial to consult a\n",
            " doctor.\n",
            "You: what are the symptoms of pancreatic cancer\n",
            "Health Assistant:\n",
            "Common symptoms of pancreatic cancer:\n",
            "\n",
            " Abdominal pain (upper left side)\n",
            "\n",
            " Back pain\n",
            " Jaundice (yellowing of skin and eyes)\n",
            " Weight loss\n",
            " Fatigue\n",
            " Loss of appetite\n",
            " Nausea and\n",
            " vomiting\n",
            "\n",
            "Other possible symptoms:\n",
            "\n",
            " Dark-colored urine\n",
            " Light-colored stools\n",
            " Diabetes\n",
            " Blood clots\n",
            " Depression\n",
            "\n",
            "It's important to note that these symptoms can also be caused by other conditions. If you experience any of these symptoms, it's crucial to consult a doctor for\n",
            " proper diagnosis and treatment.\n",
            "You: quit\n",
            "Exiting the chat. Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YOX_ZsVnxU9W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}